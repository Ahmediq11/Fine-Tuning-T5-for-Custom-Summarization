{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è 1. Environment Setup\n",
        "\n",
        "This first cell prepares our environment by installing all the necessary libraries. In addition to the standard `transformers` and `datasets` libraries, we install:\n",
        "\n",
        "- **`sentencepiece`**: A crucial dependency for the T5 model's tokenizer.\n",
        "- **`py7zr`**: A library to handle `.7z` compressed files, which might be used for downloading and decompressing certain datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U accelerate\n",
        "!pip install -U datasets\n",
        "!pip install -U bertviz\n",
        "!pip install -U umap-learn\n",
        "!pip install -U sentencepiece\n",
        "!pip install -U urllib3\n",
        "!pip install py7zr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import warnings\n", "warnings.filterwarnings('ignore')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• 2. Loading and Exploring the Dataset\n",
        "\n",
        "We use `load_dataset` to download the `samsum` dataset, which contains a collection of dialogues and their corresponding human-written summaries. This is the data we will use to teach our T5 model how to summarize conversations.\n",
        "\n",
        "After loading, we inspect the first element of the training set to understand its structure. We can see it's a dictionary with three keys: `id`, `dialogue`, and `summary`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "samsum = load_dataset('knkarthick/samsum', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["samsum['train'][0]"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 3. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Before training, it's good practice to understand the data. We calculate the length (in number of words) for both the dialogues and the summaries in our training set. Then, we plot histograms to visualize the distribution of these lengths. This helps us see that the dialogues are generally much longer than the summaries, which is what we would expect for a summarization task. This analysis can also help in choosing appropriate maximum length parameters during tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dialogue_len = [len(x['dialogue'].split()) for x in samsum['train'] if x['dialogue'] is not None]\n",
        "summary_len = [len(x['summary'].split()) for x in samsum['train'] if x['summary'] is not None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame([dialogue_len, summary_len]).T\n",
        "data.columns = ['Dialogue Length', 'Summary Length']\n",
        "\n",
        "data.hist(figsize=(10,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ 4. Loading the T5 Model and Tokenizer\n",
        "\n",
        "We are using a T5 (Text-to-Text Transfer Transformer) model, which is exceptionally good at sequence-to-sequence tasks like summarization. We load the `t5-large` checkpoint for higher performance.\n",
        "\n",
        "- **`AutoTokenizer`**: Loads the specific tokenizer that corresponds to the T5 model.\n",
        "- **`AutoModelForSeq2SeqLM`**: Loads the pre-trained T5 model architecture, ready for fine-tuning on our summarization task. The model is then moved to the GPU (`cuda`) if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# model_ckpt = 't5-small'\n",
        "model_ckpt = 't5-large'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è 5. Data Preprocessing and Tokenization\n",
        "\n",
        "This is a critical step. We define a `tokenize` function to convert our text into numerical IDs. For sequence-to-sequence models, we must tokenize both the input (`dialogue`) and the output (`summary`) at the same time. The tokenizer handles this elegantly via the `text_target` argument. We also set a `max_length` to truncate long examples.\n",
        "\n",
        "Before tokenizing, we filter the dataset to remove any rows that might have missing dialogues or summaries. Finally, we apply our `tokenize` function to the entire dataset using the efficient `.map()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    encoding = tokenizer(batch['dialogue'], text_target=batch['summary'], max_length=200, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_none_values(example):\n",
        "    return example['dialogue'] is not None and example['summary'] is not None\n",
        "samsum_clean = samsum.filter(filter_none_values)\n",
        "samsum_pt = samsum_clean.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ 6. Setting up the Trainer\n",
        "\n",
        "We configure the training process using the Hugging Face `Trainer` API.\n",
        "\n",
        "1.  **`DataCollatorForSeq2Seq`**: This is a special class that intelligently prepares batches of data for sequence-to-sequence models. It dynamically pads the input sequences and the label (summary) sequences independently, which is highly efficient.\n",
        "2.  **`TrainingArguments`**: Here we define all the hyperparameters. Notably, we set a small `per_device_train_batch_size` of 2, but use `gradient_accumulation_steps=100`. This technique allows us to simulate a much larger effective batch size (2 * 100 = 200), which stabilizes training without requiring a huge amount of GPU memory. We also enable `fp16=True` for faster mixed-precision training.\n",
        "3.  **`Trainer`**: Finally, we instantiate the `Trainer`, bringing together our model, arguments, tokenizer, data collator, and datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"train_dir\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=2e-5,\n",
        "    gradient_accumulation_steps=100,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                  args=args,\n",
        "                  tokenizer=tokenizer,\n",
        "                  data_collator=data_collator,\n",
        "                  train_dataset=samsum_pt['train'],\n",
        "                  eval_dataset=samsum_pt['validation']\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ñ∂Ô∏è 7. Training and Saving the Model\n",
        "\n",
        "With all the setup complete, we begin the training process by simply calling `trainer.train()`. The `Trainer` handles the entire fine-tuning loop, including feeding batches to the model, calculating loss, updating weights, and evaluating on the validation set.\n",
        "\n",
        "Once training is finished, we save our custom-tuned summarization model to a directory using `trainer.save_model()`. This allows us to easily reload and use it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["trainer.train()"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["trainer.save_model(\"t5_samsum_summarization\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßê 8. Inference with the Fine-Tuned Model\n",
        "\n",
        "The final step is to test our new summarization model. We load it into a `summarization` `pipeline`, which provides a very simple interface for inference. We write a new, custom dialogue and pass it to the pipeline. The output is the summary generated by our fine-tuned T5 model, demonstrating its ability to summarize conversations it has never seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["from transformers import pipeline"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe = pipeline('summarization', model='t5_samsum_summarization', device=device)\n",
        "\n",
        "custom_dialogue=\"\"\"\n",
        "Laxmi Kant: what work you planning to give Tom?\n",
        "Juli: i was hoping to send him on a business trip first.\n",
        "Laxmi Kant: cool. is there any suitable work for him?\n",
        "Juli: he did excellent in last quarter. i will assign new project, once he is back.\n",
        "\"\"\"\n",
        "\n",
        "output = pipe(custom_dialogue)\n",
        "output"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
